"""
Custom Flower strategy that logs aggregated evaluation metrics to disk.
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np

from flwr.common import Metrics
from flwr.common import ndarrays_to_parameters, parameters_to_ndarrays
from flwr.common.typing import EvaluateRes
from flwr.server.strategy import FedAvg


class LoggedFedAvg(FedAvg):
    def __init__(self, log_file: str = "state/metrics.json", *args, **kwargs) -> None:
        model_log_path = kwargs.pop("model_log_path", "state/latest_model.npz")
        super().__init__(*args, **kwargs)
        self.log_path = Path(log_file)
        self.log_path.parent.mkdir(parents=True, exist_ok=True)
        self._initialize_log_file()
        self.model_log_path: Optional[Path] = Path(model_log_path) if model_log_path else None
        if self.model_log_path:
            self.model_log_path.parent.mkdir(parents=True, exist_ok=True)

    def _initialize_log_file(self) -> None:
        initial = {"rounds": [], "loss": [], "accuracy": [], "precision": [], "recall": [], "f1": []}
        self.log_path.write_text(json.dumps(initial))

    def aggregate_evaluate(
        self,
        server_round: int,
        results: List[Tuple[EvaluateRes, Metrics]],
        failures: List[BaseException],
    ) -> Tuple[Optional[float], Dict[str, float]]:
        aggregated_loss, aggregated_metrics = super().aggregate_evaluate(
            server_round, results, failures
        )
        if aggregated_loss is not None:
            self._append_metrics(server_round, aggregated_loss, aggregated_metrics or {})
        return aggregated_loss, aggregated_metrics

    def aggregate_fit(self, server_round, results, failures):
        aggregated_parameters, aggregated_metrics = super().aggregate_fit(server_round, results, failures)
        if aggregated_parameters is not None:
            ndarrays = parameters_to_ndarrays(aggregated_parameters)
            self._save_aggregated_model(server_round, ndarrays)
        return aggregated_parameters, aggregated_metrics

    def _append_metrics(self, round_num: int, loss: float, metrics: Dict[str, float]) -> None:
        try:
            current = json.loads(self.log_path.read_text())
            current["rounds"].append(round_num)
            current["loss"].append(loss)
            current["accuracy"].append(metrics.get("accuracy", 0.0))
            current["precision"].append(metrics.get("precision", 0.0))
            current["recall"].append(metrics.get("recall", 0.0))
            current["f1"].append(metrics.get("f1", 0.0))
            self.log_path.write_text(json.dumps(current))
        except Exception as exc:  # pragma: no cover - log best effort
            print(f"[LoggedFedAvg] Failed to write metrics: {exc}")

    def _save_aggregated_model(self, round_num: int, ndarrays) -> None:
        if not self.model_log_path:
            return
        try:
            payload = {f"p{i}": arr for i, arr in enumerate(ndarrays)}
            np.savez_compressed(
                self.model_log_path,
                round=np.array([round_num], dtype=np.int64),
                **payload,
            )
        except Exception as exc:  # pragma: no cover - log best effort
            print(f"[LoggedFedAvg] Failed to save model: {exc}")


class RobustLoggedFedAvg(LoggedFedAvg):
    """
    FedAvg with optional robust aggregation (median/trimmed mean/Krum) and a simple
    FLANDERS-like z-score filter on update norms.
    """

    def __init__(
        self,
        log_file: str = "state/metrics.json",
        agg_method: str = "fedavg",
        trim_ratio: float = 0.1,
        krum_f: int = 1,
        flanders_z: Optional[float] = None,
        *args,
        **kwargs,
    ) -> None:
        super().__init__(log_file=log_file, *args, **kwargs)
        self.agg_method = agg_method.lower()
        self.trim_ratio = trim_ratio
        self.krum_f = krum_f
        self.flanders_z = flanders_z
        self._history: Dict[str, List[float]] = {}

    def aggregate_fit(
        self,
        server_round: int,
        results,
        failures,
    ):
        if not results:
            return None, {}

        # Extract parameters and weights
        weights_results = []
        filtered_results = []
        for client, fit_res in results:
            cid = str(getattr(client, "cid", "unknown"))
            ndarrays = parameters_to_ndarrays(fit_res.parameters)
            # FLANDERS-like norm filter
            if self._is_outlier(cid, ndarrays):
                print(f"[RobustLoggedFedAvg] Dropping outlier update from client {cid}")
                continue
            self._record_update(cid, ndarrays)
            weights_results.append((ndarrays, fit_res.num_examples))
            filtered_results.append((client, fit_res))

        if not weights_results:
            return None, {}

        aggregated_ndarrays = self._aggregate(weights_results)
        aggregated_parameters = ndarrays_to_parameters(aggregated_ndarrays)
        self._save_aggregated_model(server_round, aggregated_ndarrays)

        # Aggregate metrics if a fn is provided (mirrors FedAvg behavior)
        metrics_agg_fn = self.fit_metrics_aggregation_fn
        metrics = (
            metrics_agg_fn(
                [(res.num_examples, res.metrics) for _, res in filtered_results]
            )
            if metrics_agg_fn is not None
            else {}
        )
        return aggregated_parameters, metrics

    def _aggregate(self, weights_results):
        """Apply selected aggregation method."""
        params_list = [p for p, _ in weights_results]
        num_examples = np.array([float(w) for _, w in weights_results])
        if self.agg_method == "median":
            return self._agg_median(params_list)
        if self.agg_method == "trimmed":
            return self._agg_trimmed_mean(params_list)
        if self.agg_method == "krum":
            return self._agg_krum(params_list, f=self.krum_f)
        # Default FedAvg weighted mean
        return self._agg_fedavg(params_list, num_examples)

    @staticmethod
    def _agg_fedavg(params_list, weights):
        total = np.sum(weights)
        return [
            np.sum([w_i * p_i for w_i, p_i in zip(weights, layer_list)], axis=0) / total
            for layer_list in zip(*params_list)
        ]

    @staticmethod
    def _agg_median(params_list):
        return [np.median(np.stack(layer_list, axis=0), axis=0) for layer_list in zip(*params_list)]

    def _agg_trimmed_mean(self, params_list):
        n = len(params_list)
        k = int(self.trim_ratio * n)
        if k == 0 or 2 * k >= n:
            return self._agg_median(params_list)
        aggregated = []
        for layer_list in zip(*params_list):
            stacked = np.sort(np.stack(layer_list, axis=0), axis=0)
            trimmed = stacked[k : n - k]
            aggregated.append(np.mean(trimmed, axis=0))
        return aggregated

    @staticmethod
    def _agg_krum(params_list, f: int):
        n = len(params_list)
        if n == 1:
            return params_list[0]
        flat = [np.concatenate([p_layer.ravel() for p_layer in p]) for p in params_list]
        dists = np.zeros((n, n))
        for i in range(n):
            for j in range(i + 1, n):
                d = np.linalg.norm(flat[i] - flat[j]) ** 2
                dists[i, j] = dists[j, i] = d
        m = max(n - f - 2, 1)
        scores = []
        for i in range(n):
            closest = np.sort(dists[i])[: m + 1]  # include self distance 0
            scores.append(np.sum(closest))
        winner = int(np.argmin(scores))
        return params_list[winner]

    def _is_outlier(self, cid: str, ndarrays) -> bool:
        if self.flanders_z is None:
            return False
        flat = np.concatenate([arr.ravel() for arr in ndarrays])
        norm = float(np.linalg.norm(flat))
        history = self._history.get(cid, [])
        if len(history) < 3:
            return False
        mean = float(np.mean(history))
        std = float(np.std(history))
        if std == 0.0:
            return False
        z = abs(norm - mean) / std
        return z > self.flanders_z

    def _record_update(self, cid: str, ndarrays) -> None:
        flat = np.concatenate([arr.ravel() for arr in ndarrays])
        norm = float(np.linalg.norm(flat))
        self._history.setdefault(cid, []).append(norm)


class CatBoostLoggedFedAvg(LoggedFedAvg):
    """
    Federated Learning strategy for CatBoost (tree-based) models.
    
    Unlike gradient-based models, CatBoost models are serialized as byte arrays
    and cannot be averaged. This strategy picks the model from the client with
    the most training examples each round.
    
    Alternative approaches (not implemented here):
    - Tree bagging: concatenate trees from all clients
    - Ensemble voting: keep all client models and use majority voting
    """

    def aggregate_fit(self, server_round, results, failures):
        if not results:
            return None, {}

        # Find client with most training examples
        best_fit_res = None
        best_num_examples = 0
        
        for _, fit_res in results:
            if fit_res.num_examples > best_num_examples:
                best_num_examples = fit_res.num_examples
                best_fit_res = fit_res
        
        if best_fit_res is None:
            return None, {}
        
        # Use the best client's model as the global model
        aggregated_parameters = best_fit_res.parameters
        ndarrays = parameters_to_ndarrays(aggregated_parameters)
        self._save_aggregated_model(server_round, ndarrays)
        
        # Aggregate metrics
        metrics_agg_fn = self.fit_metrics_aggregation_fn
        metrics = (
            metrics_agg_fn(
                [(res.num_examples, res.metrics) for _, res in results]
            )
            if metrics_agg_fn is not None
            else {}
        )
        
        print(f"[CatBoostLoggedFedAvg] Round {server_round}: Selected model from client with {best_num_examples} examples")
        return aggregated_parameters, metrics
