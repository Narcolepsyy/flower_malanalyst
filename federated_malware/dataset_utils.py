"""
Data loading and partitioning utilities for the malware FL demo.
"""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Tuple

import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.preprocessing import StandardScaler


@dataclass
class DatasetPartition:
    train_x: np.ndarray
    train_y: np.ndarray
    val_x: np.ndarray
    val_y: np.ndarray


def _load_malmem_dataframe(csv_path: Path) -> pd.DataFrame:
    df = pd.read_csv(csv_path)
    if "Class" not in df.columns:
        raise ValueError("Expected 'Class' column in MalMem dataset")
    return df


def load_malmem(
    csv_path: str | Path = "Obfuscated-MalMem2022.csv",
) -> Tuple[np.ndarray, np.ndarray, StandardScaler]:
    """Load the MalMem dataset and return scaled features/labels."""
    df = _load_malmem_dataframe(Path(csv_path))

    features = df.drop(columns=["Class", "Category"], errors="ignore")
    labels = df["Class"].apply(lambda cls: 0 if str(cls).lower() == "benign" else 1)

    scaler = StandardScaler()
    x_scaled = scaler.fit_transform(features.values.astype(float))
    y = labels.to_numpy(dtype=np.int64)
    return x_scaled, y, scaler


def create_partitions(
    x: np.ndarray,
    y: np.ndarray,
    num_clients: int,
    val_ratio: float = 0.1,
    test_ratio: float = 0.1,
    seed: int = 42,
) -> Tuple[Dict[int, DatasetPartition], Tuple[np.ndarray, np.ndarray]]:
    """
    Split the dataset into client-specific partitions plus a shared test set.

    StratifiedKFold keeps class balance across clients, which is important for FL.
    """
    if num_clients < 1:
        raise ValueError("num_clients must be at least 1")

    x_train, x_test, y_train, y_test = train_test_split(
        x, y, test_size=test_ratio, random_state=seed, stratify=y
    )

    partitions: Dict[int, DatasetPartition] = {}
    if num_clients == 1:
        x_c_train, x_c_val, y_c_train, y_c_val = train_test_split(
            x_train,
            y_train,
            test_size=val_ratio,
            random_state=seed,
            stratify=y_train,
        )
        partitions[0] = DatasetPartition(
            train_x=x_c_train, train_y=y_c_train, val_x=x_c_val, val_y=y_c_val
        )
    else:
        skf = StratifiedKFold(n_splits=num_clients, shuffle=True, random_state=seed)
        for cid, (_, client_idx) in enumerate(skf.split(x_train, y_train)):
            x_client = x_train[client_idx]
            y_client = y_train[client_idx]

            x_c_train, x_c_val, y_c_train, y_c_val = train_test_split(
                x_client,
                y_client,
                test_size=val_ratio,
                random_state=seed,
                stratify=y_client,
            )
            partitions[cid] = DatasetPartition(
                train_x=x_c_train,
                train_y=y_c_train,
                val_x=x_c_val,
                val_y=y_c_val,
            )

    return partitions, (x_test, y_test)


def create_noniid_partitions(
    x: np.ndarray,
    y: np.ndarray,
    num_clients: int,
    alpha: float = 0.5,
    val_ratio: float = 0.1,
    test_ratio: float = 0.1,
    seed: int = 42,
) -> Tuple[Dict[int, DatasetPartition], Tuple[np.ndarray, np.ndarray]]:
    """
    Create Non-IID partitions using Dirichlet distribution.
    
    This simulates real-world FL scenarios where different clients have
    varying class distributions (e.g., one organization sees mostly ransomware,
    another sees mostly trojans).
    
    Args:
        x: Feature matrix
        y: Label array (binary: 0=benign, 1=malware)
        num_clients: Number of client partitions to create
        alpha: Dirichlet concentration parameter. Lower values create more
               heterogeneous distributions:
               - alpha=0.1: Highly skewed (some clients may have mostly one class)
               - alpha=0.5: Moderate heterogeneity
               - alpha=1.0: Mild heterogeneity
               - alpha=10.0: Nearly IID
        val_ratio: Fraction of each client's data to use for validation
        test_ratio: Fraction of total data to hold out for global testing
        seed: Random seed for reproducibility
    
    Returns:
        partitions: Dict mapping client_id -> DatasetPartition
        test_set: (x_test, y_test) tuple for global evaluation
    """
    if num_clients < 1:
        raise ValueError("num_clients must be at least 1")
    if alpha <= 0:
        raise ValueError("alpha must be positive")
    
    rng = np.random.default_rng(seed)
    
    # Split off test set first
    x_train, x_test, y_train, y_test = train_test_split(
        x, y, test_size=test_ratio, random_state=seed, stratify=y
    )
    
    # Get indices for each class
    classes = np.unique(y_train)
    class_indices = {c: np.where(y_train == c)[0] for c in classes}
    
    # Sample Dirichlet distribution for each class
    # This determines what fraction of each class goes to each client
    client_indices: Dict[int, list] = {i: [] for i in range(num_clients)}
    
    for c in classes:
        indices = class_indices[c]
        rng.shuffle(indices)
        
        # Sample proportions from Dirichlet
        proportions = rng.dirichlet([alpha] * num_clients)
        
        # Convert proportions to actual counts
        proportions = proportions / proportions.sum()  # Normalize
        counts = (proportions * len(indices)).astype(int)
        
        # Distribute any remaining samples due to rounding
        remainder = len(indices) - counts.sum()
        for i in range(remainder):
            counts[i % num_clients] += 1
        
        # Assign indices to clients
        current = 0
        for client_id, count in enumerate(counts):
            client_indices[client_id].extend(indices[current:current + count])
            current += count
    
    # Create partitions
    partitions: Dict[int, DatasetPartition] = {}
    for cid in range(num_clients):
        idx = np.array(client_indices[cid])
        if len(idx) == 0:
            # Handle edge case where client gets no data
            continue
        
        x_client = x_train[idx]
        y_client = y_train[idx]
        
        # Ensure we have both classes for stratified split
        unique_classes = np.unique(y_client)
        if len(unique_classes) < 2 or len(y_client) < 4:
            # Can't do stratified split, use random
            n_val = max(1, int(len(y_client) * val_ratio))
            perm = rng.permutation(len(y_client))
            val_idx, train_idx = perm[:n_val], perm[n_val:]
            x_c_train, y_c_train = x_client[train_idx], y_client[train_idx]
            x_c_val, y_c_val = x_client[val_idx], y_client[val_idx]
        else:
            x_c_train, x_c_val, y_c_train, y_c_val = train_test_split(
                x_client,
                y_client,
                test_size=val_ratio,
                random_state=seed,
                stratify=y_client,
            )
        
        partitions[cid] = DatasetPartition(
            train_x=x_c_train,
            train_y=y_c_train,
            val_x=x_c_val,
            val_y=y_c_val,
        )
    
    return partitions, (x_test, y_test)


def get_partition_stats(partitions: Dict[int, DatasetPartition]) -> Dict[int, Dict]:
    """Get class distribution statistics for each partition (useful for Non-IID analysis)."""
    stats = {}
    for cid, part in partitions.items():
        total = len(part.train_y) + len(part.val_y)
        malware = int((part.train_y == 1).sum() + (part.val_y == 1).sum())
        benign = total - malware
        stats[cid] = {
            "total": total,
            "malware": malware,
            "benign": benign,
            "malware_ratio": malware / total if total > 0 else 0,
        }
    return stats


def load_feature_names(csv_path: str | Path = "Obfuscated-MalMem2022.csv") -> list[str]:
    """Return ordered feature names from the MalMem CSV (excluding labels).

    Useful for explainability to map weights/attributions back to columns.
    """
    df = _load_malmem_dataframe(Path(csv_path))
    features = df.drop(columns=["Class", "Category"], errors="ignore")
    return list(features.columns)
