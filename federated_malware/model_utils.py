"""
Lightweight NumPy-based logistic regression for Flower NumPyClient.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, List, Sequence

import numpy as np
import torch
import torch.nn as nn
from sklearn.metrics import accuracy_score, log_loss, precision_recall_fscore_support

ParameterList = List[np.ndarray]


@dataclass
class TrainConfig:
    lr: float = 0.01
    epochs: int = 1
    batch_size: int = 64
    hidden1: int = 128
    hidden2: int = 64


def _compute_metrics(y_true: np.ndarray, probs: np.ndarray) -> Dict[str, float]:
    preds = (probs >= 0.5).astype(np.int64)
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true, preds, average="binary", zero_division=0
    )
    loss = log_loss(y_true, probs, labels=[0, 1])
    acc = accuracy_score(y_true, preds)
    return {
        "loss": float(loss),
        "accuracy": float(acc),
        "precision": float(precision),
        "recall": float(recall),
        "f1": float(f1),
    }


class NumpyLogisticModel:
    """Binary logistic regression with manual SGD updates."""

    def __init__(self, n_features: int, lr: float = 0.01):
        self.lr = lr
        self.weights = np.zeros((n_features,), dtype=np.float64)
        self.bias = 0.0

    def get_parameters(self) -> ParameterList:
        return [self.weights.copy(), np.array([self.bias], dtype=np.float64)]

    def set_parameters(self, parameters: Sequence[np.ndarray]) -> None:
        self.weights = parameters[0].copy()
        self.bias = float(parameters[1].reshape(-1)[0])

    def _predict_raw(self, x: np.ndarray) -> np.ndarray:
        return x @ self.weights + self.bias

    def predict_proba(self, x: np.ndarray) -> np.ndarray:
        logits = self._predict_raw(x)
        probs = 1.0 / (1.0 + np.exp(-logits))
        return probs

    def predict(self, x: np.ndarray) -> np.ndarray:
        return (self.predict_proba(x) >= 0.5).astype(np.int64)

    def fit_batch(self, x: np.ndarray, y: np.ndarray) -> float:
        preds = self.predict_proba(x)
        # Gradient for logistic loss
        error = preds - y
        grad_w = x.T @ error / len(x)
        grad_b = np.mean(error)
        self.weights -= self.lr * grad_w
        self.bias -= self.lr * grad_b
        # Small epsilon to avoid log(0)
        loss = log_loss(y, preds, labels=[0, 1])
        return float(loss)

    def train_epochs(self, x: np.ndarray, y: np.ndarray, cfg: TrainConfig) -> Dict[str, float]:
        losses: List[float] = []
        batch_size = max(1, cfg.batch_size)
        for _ in range(cfg.epochs):
            indices = np.random.permutation(len(x))
            for start in range(0, len(x), batch_size):
                end = start + batch_size
                batch_idx = indices[start:end]
                loss = self.fit_batch(x[batch_idx], y[batch_idx])
                losses.append(loss)
        return {"loss": float(np.mean(losses)) if losses else 0.0}

    def evaluate(self, x: np.ndarray, y: np.ndarray) -> Dict[str, float]:
        if len(x) == 0:
            return {"loss": 0.0, "accuracy": 0.0, "precision": 0.0, "recall": 0.0, "f1": 0.0}
        probs = self.predict_proba(x)
        return _compute_metrics(y, probs)


class TorchMLPModel:
    """Small MLP using PyTorch, exported/imported as NumPy parameters."""

    def __init__(self, n_features: int, lr: float = 1e-3, hidden1: int = 128, hidden2: int = 64):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = nn.Sequential(
            nn.Linear(n_features, hidden1),
            nn.ReLU(),
            nn.Linear(hidden1, hidden2),
            nn.ReLU(),
            nn.Linear(hidden2, 1),
        ).to(self.device)
        self.opt = torch.optim.Adam(self.model.parameters(), lr=lr)
        self.loss_fn = nn.BCEWithLogitsLoss()

    def get_parameters(self) -> ParameterList:
        return [p.detach().cpu().numpy() for p in self.model.state_dict().values()]

    def set_parameters(self, parameters: Sequence[np.ndarray]) -> None:
        state_dict = self.model.state_dict()
        new_state = {k: torch.tensor(v) for k, v in zip(state_dict.keys(), parameters)}
        self.model.load_state_dict(new_state, strict=True)

    def _predict_logits(self, x: np.ndarray) -> np.ndarray:
        self.model.eval()
        with torch.no_grad():
            xb = torch.tensor(x, dtype=torch.float32, device=self.device)
            logits = self.model(xb).cpu().numpy().reshape(-1)
        return logits

    def predict_proba(self, x: np.ndarray) -> np.ndarray:
        logits = self._predict_logits(x)
        return 1.0 / (1.0 + np.exp(-logits))

    def train_epochs(self, x: np.ndarray, y: np.ndarray, cfg: TrainConfig) -> Dict[str, float]:
        ds = torch.utils.data.TensorDataset(
            torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1)
        )
        loader = torch.utils.data.DataLoader(ds, batch_size=cfg.batch_size, shuffle=True)

        self.model.train()
        total_loss = 0.0
        for _ in range(cfg.epochs):
            for xb, yb in loader:
                xb, yb = xb.to(self.device), yb.to(self.device)
                self.opt.zero_grad()
                logits = self.model(xb)
                loss = self.loss_fn(logits, yb)
                loss.backward()
                self.opt.step()
                total_loss += loss.item() * len(xb)
        return {"loss": total_loss / max(len(ds), 1)}

    def evaluate(self, x: np.ndarray, y: np.ndarray) -> Dict[str, float]:
        if len(x) == 0:
            return {"loss": 0.0, "accuracy": 0.0, "precision": 0.0, "recall": 0.0, "f1": 0.0}
        probs = self.predict_proba(x)
        return _compute_metrics(y, probs)
