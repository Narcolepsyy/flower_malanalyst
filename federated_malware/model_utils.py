"""
Lightweight NumPy-based logistic regression for Flower NumPyClient.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, List, Sequence

import numpy as np
import torch
import torch.nn as nn
from sklearn.metrics import accuracy_score, log_loss, precision_recall_fscore_support

ParameterList = List[np.ndarray]


@dataclass
class TrainConfig:
    lr: float = 0.01
    epochs: int = 1
    batch_size: int = 64
    hidden1: int = 128
    hidden2: int = 64


def _compute_metrics(y_true: np.ndarray, probs: np.ndarray) -> Dict[str, float]:
    preds = (probs >= 0.5).astype(np.int64)
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true, preds, average="binary", zero_division=0
    )
    loss = log_loss(y_true, probs, labels=[0, 1])
    acc = accuracy_score(y_true, preds)
    return {
        "loss": float(loss),
        "accuracy": float(acc),
        "precision": float(precision),
        "recall": float(recall),
        "f1": float(f1),
    }


class NumpyLogisticModel:
    """Binary logistic regression with manual SGD updates."""

    def __init__(self, n_features: int, lr: float = 0.01):
        self.lr = lr
        self.weights = np.zeros((n_features,), dtype=np.float64)
        self.bias = 0.0

    def get_parameters(self) -> ParameterList:
        return [self.weights.copy(), np.array([self.bias], dtype=np.float64)]

    def set_parameters(self, parameters: Sequence[np.ndarray]) -> None:
        self.weights = parameters[0].copy()
        self.bias = float(parameters[1].reshape(-1)[0])

    def _predict_raw(self, x: np.ndarray) -> np.ndarray:
        return x @ self.weights + self.bias

    def predict_proba(self, x: np.ndarray) -> np.ndarray:
        logits = self._predict_raw(x)
        probs = 1.0 / (1.0 + np.exp(-logits))
        return probs

    def predict(self, x: np.ndarray) -> np.ndarray:
        return (self.predict_proba(x) >= 0.5).astype(np.int64)

    def fit_batch(self, x: np.ndarray, y: np.ndarray) -> float:
        preds = self.predict_proba(x)
        # Gradient for logistic loss
        error = preds - y
        grad_w = x.T @ error / len(x)
        grad_b = np.mean(error)
        self.weights -= self.lr * grad_w
        self.bias -= self.lr * grad_b
        # Small epsilon to avoid log(0)
        loss = log_loss(y, preds, labels=[0, 1])
        return float(loss)

    def train_epochs(self, x: np.ndarray, y: np.ndarray, cfg: TrainConfig) -> Dict[str, float]:
        losses: List[float] = []
        batch_size = max(1, cfg.batch_size)
        for _ in range(cfg.epochs):
            indices = np.random.permutation(len(x))
            for start in range(0, len(x), batch_size):
                end = start + batch_size
                batch_idx = indices[start:end]
                loss = self.fit_batch(x[batch_idx], y[batch_idx])
                losses.append(loss)
        return {"loss": float(np.mean(losses)) if losses else 0.0}

    def evaluate(self, x: np.ndarray, y: np.ndarray) -> Dict[str, float]:
        if len(x) == 0:
            return {"loss": 0.0, "accuracy": 0.0, "precision": 0.0, "recall": 0.0, "f1": 0.0}
        probs = self.predict_proba(x)
        return _compute_metrics(y, probs)


class TorchMLPModel:
    """Small MLP using PyTorch, exported/imported as NumPy parameters."""

    def __init__(self, n_features: int, lr: float = 1e-3, hidden1: int = 128, hidden2: int = 64):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = nn.Sequential(
            nn.Linear(n_features, hidden1),
            nn.ReLU(),
            nn.Linear(hidden1, hidden2),
            nn.ReLU(),
            nn.Linear(hidden2, 1),
        ).to(self.device)
        self.opt = torch.optim.Adam(self.model.parameters(), lr=lr)
        self.loss_fn = nn.BCEWithLogitsLoss()

    def get_parameters(self) -> ParameterList:
        return [p.detach().cpu().numpy() for p in self.model.state_dict().values()]

    def set_parameters(self, parameters: Sequence[np.ndarray]) -> None:
        state_dict = self.model.state_dict()
        new_state = {k: torch.tensor(v) for k, v in zip(state_dict.keys(), parameters)}
        self.model.load_state_dict(new_state, strict=True)

    def _predict_logits(self, x: np.ndarray) -> np.ndarray:
        self.model.eval()
        with torch.no_grad():
            xb = torch.tensor(x, dtype=torch.float32, device=self.device)
            logits = self.model(xb).cpu().numpy().reshape(-1)
        return logits

    def predict_proba(self, x: np.ndarray) -> np.ndarray:
        logits = self._predict_logits(x)
        return 1.0 / (1.0 + np.exp(-logits))

    def train_epochs(self, x: np.ndarray, y: np.ndarray, cfg: TrainConfig) -> Dict[str, float]:
        ds = torch.utils.data.TensorDataset(
            torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1)
        )
        loader = torch.utils.data.DataLoader(ds, batch_size=cfg.batch_size, shuffle=True)

        self.model.train()
        total_loss = 0.0
        for _ in range(cfg.epochs):
            for xb, yb in loader:
                xb, yb = xb.to(self.device), yb.to(self.device)
                self.opt.zero_grad()
                logits = self.model(xb)
                loss = self.loss_fn(logits, yb)
                loss.backward()
                self.opt.step()
                total_loss += loss.item() * len(xb)
        return {"loss": total_loss / max(len(ds), 1)}

    def evaluate(self, x: np.ndarray, y: np.ndarray) -> Dict[str, float]:
        if len(x) == 0:
            return {"loss": 0.0, "accuracy": 0.0, "precision": 0.0, "recall": 0.0, "f1": 0.0}
        probs = self.predict_proba(x)
        return _compute_metrics(y, probs)


class DPTorchMLPModel:
    """
    Differentially Private MLP using Opacus.
    
    This model wraps the training process with Opacus PrivacyEngine to provide
    formal differential privacy guarantees. The privacy budget (epsilon, delta)
    controls the privacy-utility trade-off.
    
    Lower epsilon = stronger privacy but potentially lower accuracy.
    """

    def __init__(
        self,
        n_features: int,
        lr: float = 1e-3,
        hidden1: int = 128,
        hidden2: int = 64,
        target_epsilon: float = 1.0,
        target_delta: float = 1e-5,
        max_grad_norm: float = 1.0,
        noise_multiplier: float = 1.0,
    ):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.target_epsilon = target_epsilon
        self.target_delta = target_delta
        self.max_grad_norm = max_grad_norm
        self.noise_multiplier = noise_multiplier
        self.spent_epsilon = 0.0
        
        # Build model - Opacus requires specific layer types
        self.model = nn.Sequential(
            nn.Linear(n_features, hidden1),
            nn.ReLU(),
            nn.Linear(hidden1, hidden2),
            nn.ReLU(),
            nn.Linear(hidden2, 1),
        ).to(self.device)
        
        self.opt = torch.optim.Adam(self.model.parameters(), lr=lr)
        self.loss_fn = nn.BCEWithLogitsLoss()
        self._privacy_engine = None

    def get_parameters(self) -> ParameterList:
        # For DP models, we need to get the underlying model parameters
        model = self._get_underlying_model()
        return [p.detach().cpu().numpy() for p in model.state_dict().values()]

    def _get_underlying_model(self):
        """Get the actual model (unwrap from GradSampleModule if wrapped)."""
        if hasattr(self.model, '_module'):
            return self.model._module
        return self.model

    def set_parameters(self, parameters: Sequence[np.ndarray]) -> None:
        model = self._get_underlying_model()
        state_dict = model.state_dict()
        new_state = {k: torch.tensor(v) for k, v in zip(state_dict.keys(), parameters)}
        model.load_state_dict(new_state, strict=True)

    def _predict_logits(self, x: np.ndarray) -> np.ndarray:
        model = self._get_underlying_model()
        model.eval()
        with torch.no_grad():
            xb = torch.tensor(x, dtype=torch.float32, device=self.device)
            logits = model(xb).cpu().numpy().reshape(-1)
        return logits

    def predict_proba(self, x: np.ndarray) -> np.ndarray:
        logits = self._predict_logits(x)
        return 1.0 / (1.0 + np.exp(-logits))

    def train_epochs(self, x: np.ndarray, y: np.ndarray, cfg: TrainConfig) -> Dict[str, float]:
        try:
            from opacus import PrivacyEngine
            from opacus.validators import ModuleValidator
        except ImportError:
            raise ImportError("opacus is required for DPTorchMLPModel. Install with: pip install opacus")

        ds = torch.utils.data.TensorDataset(
            torch.tensor(x, dtype=torch.float32), 
            torch.tensor(y, dtype=torch.float32).unsqueeze(1)
        )
        loader = torch.utils.data.DataLoader(ds, batch_size=cfg.batch_size, shuffle=True)

        # Validate and fix model for Opacus compatibility
        model = self._get_underlying_model()
        if not ModuleValidator.is_valid(model):
            model = ModuleValidator.fix(model)
            self.model = model.to(self.device)

        # Initialize PrivacyEngine
        privacy_engine = PrivacyEngine()
        
        self.model, self.opt, loader = privacy_engine.make_private(
            module=self.model,
            optimizer=self.opt,
            data_loader=loader,
            noise_multiplier=self.noise_multiplier,
            max_grad_norm=self.max_grad_norm,
        )
        self._privacy_engine = privacy_engine

        self.model.train()
        total_loss = 0.0
        for _ in range(cfg.epochs):
            for xb, yb in loader:
                xb, yb = xb.to(self.device), yb.to(self.device)
                self.opt.zero_grad()
                logits = self.model(xb)
                loss = self.loss_fn(logits, yb)
                loss.backward()
                self.opt.step()
                total_loss += loss.item() * len(xb)

        # Track spent privacy budget
        self.spent_epsilon = privacy_engine.get_epsilon(delta=self.target_delta)
        
        return {
            "loss": total_loss / max(len(ds), 1),
            "dp_epsilon": self.spent_epsilon,
        }

    def evaluate(self, x: np.ndarray, y: np.ndarray) -> Dict[str, float]:
        if len(x) == 0:
            return {"loss": 0.0, "accuracy": 0.0, "precision": 0.0, "recall": 0.0, "f1": 0.0}
        probs = self.predict_proba(x)
        metrics = _compute_metrics(y, probs)
        metrics["dp_epsilon"] = self.spent_epsilon
        return metrics


class CatBoostModel:
    """
    CatBoost gradient boosting classifier for Federated Learning.
    
    Unlike gradient-based models (logistic regression, MLP), CatBoost is tree-based.
    For FL, we serialize/deserialize the entire model each round since tree ensembles
    cannot be averaged like neural network weights.
    
    The model is warm-started each round, continuing to add trees to the ensemble.
    """

    def __init__(
        self,
        n_features: int,
        iterations: int = 50,
        depth: int = 6,
        learning_rate: float = 0.1,
    ):
        try:
            from catboost import CatBoostClassifier
        except ImportError:
            raise ImportError("catboost is required. Install with: pip install catboost")
        
        self.n_features = n_features
        self.iterations = iterations
        self.depth = depth
        self.learning_rate = learning_rate
        
        self.model = CatBoostClassifier(
            iterations=iterations,
            depth=depth,
            learning_rate=learning_rate,
            verbose=False,
            allow_writing_files=False,
            loss_function='Logloss',
            random_seed=42,
        )
        self._is_fitted = False

    def get_parameters(self) -> ParameterList:
        """Serialize model to bytes and return as numpy array."""
        import pickle
        if not self._is_fitted:
            # Return empty placeholder if model not yet trained
            return [np.array([], dtype=np.uint8)]
        
        model_bytes = pickle.dumps(self.model)
        return [np.frombuffer(model_bytes, dtype=np.uint8)]

    def set_parameters(self, parameters: Sequence[np.ndarray]) -> None:
        """Deserialize model from bytes."""
        import pickle
        from catboost import CatBoostClassifier
        
        if len(parameters) == 0 or len(parameters[0]) == 0:
            # No model to load yet
            return
        
        model_bytes = parameters[0].tobytes()
        self.model = pickle.loads(model_bytes)
        self._is_fitted = True

    def predict_proba(self, x: np.ndarray) -> np.ndarray:
        """Return probability of positive class."""
        if not self._is_fitted:
            # Return 0.5 for unfitted model
            return np.full(len(x), 0.5)
        probs = self.model.predict_proba(x)
        return probs[:, 1] if probs.ndim > 1 else probs

    def train_epochs(self, x: np.ndarray, y: np.ndarray, cfg: TrainConfig) -> Dict[str, float]:
        """Train CatBoost model on local data."""
        from catboost import CatBoostClassifier, Pool
        
        # For subsequent rounds, continue training with warm start
        if self._is_fitted:
            # Create new model with updated iterations for warm start
            new_model = CatBoostClassifier(
                iterations=self.iterations,
                depth=self.depth,
                learning_rate=self.learning_rate,
                verbose=False,
                allow_writing_files=False,
                loss_function='Logloss',
                random_seed=42,
            )
            train_pool = Pool(x, y)
            new_model.fit(train_pool, init_model=self.model)
            self.model = new_model
        else:
            # First training
            train_pool = Pool(x, y)
            self.model.fit(train_pool)
        
        self._is_fitted = True
        
        # Compute training loss
        probs = self.predict_proba(x)
        loss = log_loss(y, probs, labels=[0, 1])
        return {"loss": float(loss)}

    def evaluate(self, x: np.ndarray, y: np.ndarray) -> Dict[str, float]:
        """Evaluate model and return metrics."""
        if len(x) == 0:
            return {"loss": 0.0, "accuracy": 0.0, "precision": 0.0, "recall": 0.0, "f1": 0.0}
        probs = self.predict_proba(x)
        return _compute_metrics(y, probs)


class HybridQuantumModel:
    """
    Hybrid quantum-classical model for Federated Learning.
    
    Architecture:
    - Classical layers: n_features → 64 → 16 → 4 (dimensionality reduction)
    - Quantum layer: 4-qubit variational circuit with angle encoding + entanglement
    - Output: Quantum measurements → sigmoid → binary classification
    
    Uses PennyLane with default.qubit simulator (runs on CPU).
    Training is slower than classical models due to quantum simulation overhead.
    """

    N_QUBITS = 4

    def __init__(self, n_features: int, lr: float = 1e-3, n_layers: int = 2):
        try:
            import pennylane as qml
        except ImportError:
            raise ImportError("pennylane is required. Install with: pip install pennylane")
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.n_features = n_features
        self.n_layers = n_layers
        self.qml = qml
        
        # Create quantum device and circuit
        dev = qml.device("default.qubit", wires=self.N_QUBITS)
        
        @qml.qnode(dev, interface="torch", diff_method="backprop")
        def quantum_circuit(inputs, weights):
            # Angle encoding - inputs should be shape (4,) for single sample
            qml.AngleEmbedding(inputs, wires=range(self.N_QUBITS))
            # Variational layers with entanglement
            qml.BasicEntanglerLayers(weights, wires=range(self.N_QUBITS))
            return qml.expval(qml.PauliZ(0))
        
        weight_shapes = {"weights": (n_layers, self.N_QUBITS)}
        self.qlayer = qml.qnn.TorchLayer(quantum_circuit, weight_shapes)
        
        # Classical encoder (reduces to N_QUBITS dimensions)
        self.encoder = nn.Sequential(
            nn.Linear(n_features, 64),
            nn.ReLU(),
            nn.Linear(64, 16),
            nn.ReLU(),
            nn.Linear(16, self.N_QUBITS),
            nn.Tanh(),  # Scale to [-π, π] range for rotation angles
        )
        
        # Final classifier
        self.classifier = nn.Linear(1, 1)
        
        # Move to device
        self.encoder = self.encoder.to(self.device)
        self.classifier = self.classifier.to(self.device)
        
        # Optimizer
        params = list(self.encoder.parameters()) + list(self.qlayer.parameters()) + list(self.classifier.parameters())
        self.opt = torch.optim.Adam(params, lr=lr)
        self.loss_fn = nn.BCEWithLogitsLoss()

    def _forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through encoder -> quantum -> classifier."""
        # Encode to quantum-compatible dimensions
        encoded = self.encoder(x)  # (batch, 4)
        encoded = encoded * 3.14159  # Scale to ~[-π, π] for rotations
        
        # Apply quantum layer (handles batches automatically)
        q_out = self.qlayer(encoded)  # (batch,) or (batch, 1)
        if q_out.dim() == 1:
            q_out = q_out.unsqueeze(1)  # (batch, 1)
        
        # Final classification
        logits = self.classifier(q_out).squeeze(-1)  # (batch,)
        return logits

    def get_parameters(self) -> ParameterList:
        params = []
        for p in self.encoder.state_dict().values():
            params.append(p.detach().cpu().numpy())
        for p in self.qlayer.state_dict().values():
            params.append(p.detach().cpu().numpy())
        for p in self.classifier.state_dict().values():
            params.append(p.detach().cpu().numpy())
        return params

    def set_parameters(self, parameters: Sequence[np.ndarray]) -> None:
        # Split parameters for encoder (6), qlayer (1), classifier (2)
        enc_keys = list(self.encoder.state_dict().keys())
        q_keys = list(self.qlayer.state_dict().keys())
        cls_keys = list(self.classifier.state_dict().keys())
        
        idx = 0
        enc_state = {k: torch.tensor(parameters[idx + i]) for i, k in enumerate(enc_keys)}
        idx += len(enc_keys)
        q_state = {k: torch.tensor(parameters[idx + i]) for i, k in enumerate(q_keys)}
        idx += len(q_keys)
        cls_state = {k: torch.tensor(parameters[idx + i]) for i, k in enumerate(cls_keys)}
        
        self.encoder.load_state_dict(enc_state, strict=True)
        self.qlayer.load_state_dict(q_state, strict=True)
        self.classifier.load_state_dict(cls_state, strict=True)

    def predict_proba(self, x: np.ndarray) -> np.ndarray:
        self.encoder.eval()
        with torch.no_grad():
            xb = torch.tensor(x, dtype=torch.float32, device=self.device)
            logits = self._forward(xb)
            probs = torch.sigmoid(logits)
            return probs.cpu().numpy()

    def train_epochs(self, x: np.ndarray, y: np.ndarray, cfg: TrainConfig) -> Dict[str, float]:
        ds = torch.utils.data.TensorDataset(
            torch.tensor(x, dtype=torch.float32),
            torch.tensor(y, dtype=torch.float32),
        )
        loader = torch.utils.data.DataLoader(ds, batch_size=cfg.batch_size, shuffle=True)

        self.encoder.train()
        total_loss = 0.0
        for _ in range(cfg.epochs):
            for xb, yb in loader:
                xb, yb = xb.to(self.device), yb.to(self.device)
                self.opt.zero_grad()
                logits = self._forward(xb)
                loss = self.loss_fn(logits, yb)
                loss.backward()
                self.opt.step()
                total_loss += loss.item() * len(xb)
        return {"loss": total_loss / max(len(ds), 1)}

    def evaluate(self, x: np.ndarray, y: np.ndarray) -> Dict[str, float]:
        if len(x) == 0:
            return {"loss": 0.0, "accuracy": 0.0, "precision": 0.0, "recall": 0.0, "f1": 0.0}
        probs = self.predict_proba(x)
        return _compute_metrics(y, probs)

